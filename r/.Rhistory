fp = sum(results$match == 1 & results$response==0)
fn = sum(results$match == 0 & results$response==1)
precision = tp/(tp+fp)
recall = tp/(tp+fn)
f1 = 2*(recall*precision)/(recall+precision)
hprec <- sum(results[results$match==1,]$hprecision) / (tp+fp)
hrec <- sum(results[results$match==1,]$hrecall) / (tp+fp)
# find average hp and hr for all
table_prt[row,"hprecision"] <- hprec
table_prt[row,"hrecall"] <- hrec
table_prt[row,"precision"] <- precision
table_prt[row,"recall"] <- recall
table_prt[row,"F1"] <- f1
table_prt[row,"actual"] <- sum(results$response == 1)
table_prt[row,"predict"] <- sum(results$match == 1)
table_prt[row,"correct"] <- tp
}
# Truncating results files.
cols <- as.vector(c("chunk","term","X1"))
results_short <- results[(results$rank <= 20),]
results_short <- results_short[,cols]
results_long <- results[,cols]
# Writing the output table files.
paste(Sys.time(),"writing table files")
write.csv(table_prr, file=table_pr_rank)
write.csv(table_prt, file=table_pr_thresh)
write.csv(table3, file=table_rankquality)
write.csv(roles_table, file=table_rolequality)
write.csv(results_short, file=resultspath_short, row.names=FALSE)
write.csv(results_long, file=resultspath_long, row.names=FALSE)
library(randomForest)
library(tidyr)
library(dplyr)
library(ggplot2)
library(lattice)
library(mice)
# Change these
datapath <- "/work/dillpicl/irbraun/term-mapping/run5/data.csv"
outputroot <- "/work/dillpicl/irbraun/term-mapping/run5/learned/"
datapath <- "/Users/irbraun/Desktop/d.csv"
outputroot <- "/Users/irbraun/Desktop/temp/test/"
# Leave these alone
table_blanks <- paste(outputroot,"blanks_table.csv",sep="")
table_varimp <- paste(outputroot,"var_imp_table.csv",sep="")
table_pr_rank <- paste(outputroot,"pr_rank_table.csv",sep="")
table_pr_thresh <- paste(outputroot,"pr_thresh_table.csv",sep="")
table_rankquality <- paste(outputroot,"rank_quality_table.csv",sep="")
table_rolequality <- paste(outputroot,"role_quality_table.csv",sep="")
forestpath <- paste(outputroot,"forest.RData",sep="")
inter_trainpath <- paste(outputroot,"training.csv",sep="")
inter_testpath <- paste(outputroot,"testing.csv",sep="")
resultspath_short <- paste(outputroot,"classified_abbr.csv",sep="")
resultspath_long <- paste(outputroot,"classified_full.csv",sep="")
# Seed value
set.seed(5791)
# Read in data from running the jar file.
paste(Sys.time(),"reading in the ouput of the jar file")
d <- read.csv(file=datapath, header=TRUE, stringsAsFactors=FALSE, sep=",")
d$response <- as.factor(d$response)
d[d==-1] <- NA
# Handle missing values, currently removed all features with >50% mising values.
paste(Sys.time(),"removing sparse features")
blank <- sapply(X=d, FUN=function(x) sum(is.na(x))/nrow(d))
write.csv(blank, file=table_blanks)
d <- d[, -which(colMeans(is.na(d)) > 0.500)]
# Partition while keeping all pairs <c,t> grouped by c value.
paste(Sys.time(),"partitioning")
num_retained <- floor(0.3*length(unique(d$chunk)))
testchunks <- sample(unique(d$chunk), num_retained)
train <- subset(d, !chunk %in% testchunks)
test <- subset(d, chunk %in% testchunks)
# Remove negative examples randomly from the training data to get desired class ratio but keep constant between chunks.
paste(Sys.time(),"removing some data")
class.ratio = 0.01
num.neg <- sum(train$response==0)
num.neg.retain <- floor(sum(train$response==1) / class.ratio)
num.delete <- num.neg - num.neg.retain
num.delete.perchunk <- floor(num.delete / length(unique(train$chunk)))
for (c in unique(train$chunk)){
vneg <- which(train$response==0 & train$chunk==c)
vneg.remove <- sample(vneg, num.delete.perchunk)
train <- train[-vneg.remove,]
}
train.medians <- sapply(train, function(x){if(is.numeric(x)){median(x, na.rm=TRUE)}})
train <- train %>% replace_na(train.medians)
test.medians <- sapply(test, function(x){if(is.numeric(x)){median(x, na.rm=TRUE)}})
test <- test %>% replace_na(test.medians)
# Save the full training and testing datasets in case of error after this point.
write.csv(train,file=inter_trainpath)
write.csv(test,file=inter_testpath)
# Set weights for each class.
pos.weight=1;
neg.weight=1;
# Build random forest.
paste(Sys.time(),"constructing trees")
forest = randomForest(response ~ . -chunk -term -hprecision -hrecall -hpmaxer -hrmaxer -role, data=train, classwt=c("1"=pos.weight, "0"=neg.weight), ntree=500, importance=T, type=classification)
paste(Sys.time(),"saving forest")
save(forest,file=forestpath)
print(forest)
library(randomForest)
library(tidyr)
library(dplyr)
library(ggplot2)
library(lattice)
library(mice)
# Change these
datapath <- "/work/dillpicl/irbraun/term-mapping/run5/data.csv"
outputroot <- "/work/dillpicl/irbraun/term-mapping/run5/learned/"
datapath <- "/Users/irbraun/Desktop/d.csv"
outputroot <- "/Users/irbraun/Desktop/temp/test/"
# Leave these alone
table_blanks <- paste(outputroot,"blanks_table.csv",sep="")
table_varimp <- paste(outputroot,"var_imp_table.csv",sep="")
table_pr_rank <- paste(outputroot,"pr_rank_table.csv",sep="")
table_pr_thresh <- paste(outputroot,"pr_thresh_table.csv",sep="")
table_rankquality <- paste(outputroot,"rank_quality_table.csv",sep="")
table_rolequality <- paste(outputroot,"role_quality_table.csv",sep="")
forestpath <- paste(outputroot,"forest.RData",sep="")
inter_trainpath <- paste(outputroot,"training.csv",sep="")
inter_testpath <- paste(outputroot,"testing.csv",sep="")
resultspath_short <- paste(outputroot,"classified_abbr.csv",sep="")
resultspath_long <- paste(outputroot,"classified_full.csv",sep="")
# Seed value
set.seed(5791)
# Read in data from running the jar file.
paste(Sys.time(),"reading in the ouput of the jar file")
d <- read.csv(file=datapath, header=TRUE, stringsAsFactors=FALSE, sep=",")
d$response <- as.factor(d$response)
d[d==-1] <- NA
# Handle missing values, currently removed all features with >50% mising values.
paste(Sys.time(),"removing sparse features")
blank <- sapply(X=d, FUN=function(x) sum(is.na(x))/nrow(d))
write.csv(blank, file=table_blanks)
d <- d[, -which(colMeans(is.na(d)) > 0.500)]
# Partition while keeping all pairs <c,t> grouped by c value.
paste(Sys.time(),"partitioning")
num_retained <- floor(0.3*length(unique(d$chunk)))
testchunks <- sample(unique(d$chunk), num_retained)
train <- subset(d, !chunk %in% testchunks)
test <- subset(d, chunk %in% testchunks)
# Remove negative examples randomly from the training data to get desired class ratio but keep constant between chunks.
paste(Sys.time(),"removing some data")
class.ratio = 0.01
num.neg <- sum(train$response==0)
num.neg.retain <- floor(sum(train$response==1) / class.ratio)
num.delete <- num.neg - num.neg.retain
num.delete.perchunk <- floor(num.delete / length(unique(train$chunk)))
for (c in unique(train$chunk)){
vneg <- which(train$response==0 & train$chunk==c)
vneg.remove <- sample(vneg, num.delete.perchunk)
train <- train[-vneg.remove,]
}
# Imputation using medians.
train.medians <- sapply(train, function(x){if(is.numeric(x)){median(x, na.rm=TRUE)}})
train <- train %>% replace_na(train.medians)
test.medians <- sapply(test, function(x){if(is.numeric(x)){median(x, na.rm=TRUE)}})
test <- test %>% replace_na(test.medians)
# Save the full training and testing datasets in case of error after this point.
write.csv(train,file=inter_trainpath)
write.csv(test,file=inter_testpath)
# Set weights for each class.
pos.weight=1;
neg.weight=1;
# Build random forest.
paste(Sys.time(),"constructing trees")
forest = randomForest(response ~ . -chunk -term -hprecision -hrecall -hpmaxer -hrmaxer -role, data=train, classwt=c("1"=pos.weight, "0"=neg.weight), ntree=500, importance=T, type=classification)
paste(Sys.time(),"saving forest")
save(forest,file=forestpath)
print(forest)
# Look at variable importance.
paste(Sys.time(),"looking at variable importance")
var.imp <- data.frame(importance(forest))
var.imp$Variables <- row.names(var.imp)
var.imp <- var.imp[order(var.imp$X1,decreasing=T),]
write.csv(var.imp, file=table_varimp)
# Predict on test data.
paste(Sys.time(),"predicting on test data")
pred <- predict(forest, test, type = "prob")
pred.df = data.frame(pred)
results = cbind(test,pred.df)
# Making tables
paste(Sys.time(),"preparing output tables")
# Determine term ranks independently for each chunk.
results <- transform(results, rank = ave(X1, chunk, FUN=function(x) rank(-x, ties.method = "random")))
# Table of average quality of each rank.
table3 <- aggregate(results[,c("hprecision","hrecall")], list(results$rank), mean)
# # Precision Recall table based off of ranking within each each chunk.
# numTerms <- 1758
# threshold <- seq(0, numTerms, 1)
# cols <- c("precision","recall","hprecision","hrecall")
# table_prr <- data.frame(threshold)
# table_prr[,cols] <- 0
# for (row in 1:nrow(table_prr)){
#   thresh <- table_prr[row,"threshold"]
#   results$match <- ifelse(results$rank <= thresh, 1, 0)
#   results$match <- as.factor(results$match)
#   tp = sum(results$match == 1 & results$response==1)
#   fp = sum(results$match == 1 & results$response==0)
#   fn = sum(results$match == 0 & results$response==1)
#   precision = tp/(tp+fp)
#   recall = tp/(tp+fn)
#
#   hprec <- sum(results[results$match==1,]$hprecision) / (tp+fp)
#   hrec <- sum(results[results$match==1,]$hrecall) / (tp+fp)
#
#   table_prr[row,"hprecision"] <- hprec
#   table_prr[row,"hrecall"] <- hrec
#   table_prr[row,"precision"] <- precision
#   table_prr[row,"recall"] <- recall
# }
# Precision Recall table based off P(w1) threshold.
threshold <- seq(0, 1, 0.01)
cols <- c("precision","recall","F1","actual","predict","correct","hprecision","hrecall")
table_prt <- data.frame(threshold)
table_prt[,cols] <- 0
for (row in 1:nrow(table_prt)){
thresh <- table_prt[row,"threshold"]
results$match <- ifelse(results$X1 >= thresh, 1, 0)
results$match <- as.factor(results$match)
tp = sum(results$match == 1 & results$response==1)
fp = sum(results$match == 1 & results$response==0)
fn = sum(results$match == 0 & results$response==1)
precision = tp/(tp+fp)
recall = tp/(tp+fn)
f1 = 2*(recall*precision)/(recall+precision)
hprec <- sum(results[results$match==1,]$hprecision) / (tp+fp)
hrec <- sum(results[results$match==1,]$hrecall) / (tp+fp)
# find average hp and hr for all
table_prt[row,"hprecision"] <- hprec
table_prt[row,"hrecall"] <- hrec
table_prt[row,"precision"] <- precision
table_prt[row,"recall"] <- recall
table_prt[row,"F1"] <- f1
table_prt[row,"actual"] <- sum(results$response == 1)
table_prt[row,"predict"] <- sum(results$match == 1)
table_prt[row,"correct"] <- tp
}
# Find the average ranks for each role in this part of the EQ statement.
roles <- c("PRIMARY_ENTITY1_ID","PRIMARY_ENTITY2_ID","SECONDARY_ENTITY1_ID","SECONDARY_ENTITY2_ID","DEVELOPMENTAL_STAGE_ID")
roles_table <- data.frame(roles)
cols <- c("mean","median")
roles_table[,cols] <- 0
for (row in 1:nrow(roles_table)){
r <- roles_table[row,"roles"]
results.role <- subset(results, results$role==r)
roles_table[row,"mean"] <- mean(results.role$rank)
roles_table[row,"median"] <- median(results.role$rank)
}
# Truncating results files.
cols <- as.vector(c("chunk","term","X1"))
results_short <- results[(results$rank <= 20),]
results_short <- results_short[,cols]
results_long <- results[,cols]
# Writing the output table files.
paste(Sys.time(),"writing table files")
#write.csv(table_prr, file=table_pr_rank)
write.csv(table_prt, file=table_pr_thresh)
write.csv(table3, file=table_rankquality)
write.csv(roles_table, file=table_rolequality)
write.csv(results_short, file=resultspath_short, row.names=FALSE)
write.csv(results_long, file=resultspath_long, row.names=FALSE)
cat(paste("n,pp,pr,pf1\n"))
dir <- "/Users/irbraun/NetBeansProjects/term-mapping/path/annotators/naive/"
f1 <- "outputs_pato/test_eval.csv"
f2 <- "outputs_po/test_eval.csv"
cat(paste(table_row(dir,f1),"\n",sep=""))
cat(paste(table_row(dir,f2),"\n",sep=""))
# Normalization function for numerical scores.
range01 <- function(x, ...){(x - min(x, ...)) / (max(x, ...) - min(x, ...))}
# Function for creating a string for input into a table.
read <- function(dir,filename){
# Read in the data.
d <- read.csv(file=paste(dir,filename,sep=""), header=T, sep=",")
return(d)
}
# Function for creating a string for input into a table.
table_row <- function(dir,filename){
# Read in the data.
d <- read.csv(file=paste(dir,filename,sep=""), header=T, sep=",")
# Numerical data that is missing was marked as na in the file.
d[d=="na"] <- NA
# Check this stuff, only applies when a score was return from the concept mapping method.
#d$score <- as.numeric(as.character(d$score))
#d$score <- range01(d$score, na.rm=T)
#d[d$category%in%c("FN"),"score"] <- NA
#threshold = 0.5
#d <- d[(d$score >= threshold | is.na(d$score)),]
# Non-hierarchical metrics.
tp <- nrow(d[d$category %in% c("TP"),])
fp <- nrow(d[d$category %in% c("FP"),])
fn <- nrow(d[d$category %in% c("FN"),])
p <- tp/(tp+fp)
r <- tp/(tp+fn)
f1 <- (2*p*r)/(p+r)
# Graph-based metrics.
sum <- sum(d[d$category %in% c("TP","FN"),]$similarity)
avg <- sum/(tp+fn)
pr <- avg
sum <- sum(d[d$category %in% c("TP","FP"),]$similarity)
avg <- sum/(tp+fp)
pp <- avg
pf1 <- (2*pp*pr)/(pr+pp)
n <- tp+fn
# Adding one for partial precision of just the 'false' positives.
sum <- sum(d[d$category %in% c("FP"),]$similarity)
avg <- sum/(fp)
pp_fp <- avg
line = paste(n,pp,pr,pf1,sep=",")
return(line)
}
# Function for finding an appropriate threshold for this method.
find_threshold <- function(dir,filename,ratio){
# Read in the data.
d <- read.csv(file=paste(dir,filename,sep=""), header=T, sep=",")
# Numerical data that is missing was marked as na in the file.
d[d=="na"] <- NA
d[d$category%in%c("FN"),"score"] <- NA
d$score <- as.numeric(as.character(d$score))
# Total number of chunks in the testing set.
num_chunks <- length(unique(d$chunk))
# What's the expected number of terms in this ontology?
expected <- num_chunks*ratio
# What threshold on the predicted term's scores would leave that many?
d_pred <- d[d$category %in% c("FP","TP"),]
sorted_pred_scores <- sort(d_pred$score, decreasing=T, na.last=NA)
threshold <- sorted_pred_scores[ceiling(expected)]
return(threshold)
}
# Get a csv with metrics for a particular semantic annotation method.
sink("/Users/irbraun/Desktop/metrics.csv")
# testing the word embedding stuff for the nb model.
cat(paste("n,pp,pr,pf1\n"))
dir <- "/Users/irbraun/NetBeansProjects/term-mapping/path/annotators/naive/"
f1 <- "outputs_pato/test_eval.csv"
f2 <- "outputs_po/test_eval.csv"
cat(paste(table_row(dir,f1),"\n",sep=""))
cat(paste(table_row(dir,f2),"\n",sep=""))
# Normalization function for numerical scores.
range01 <- function(x, ...){(x - min(x, ...)) / (max(x, ...) - min(x, ...))}
# Function for creating a string for input into a table.
read <- function(dir,filename){
# Read in the data.
d <- read.csv(file=paste(dir,filename,sep=""), header=T, sep=",")
return(d)
}
# Function for creating a string for input into a table.
table_row <- function(dir,filename){
# Read in the data.
d <- read.csv(file=paste(dir,filename,sep=""), header=T, sep=",")
# Numerical data that is missing was marked as na in the file.
d[d=="na"] <- NA
# Check this stuff, only applies when a score was return from the concept mapping method.
#d$score <- as.numeric(as.character(d$score))
#d$score <- range01(d$score, na.rm=T)
#d[d$category%in%c("FN"),"score"] <- NA
#threshold = 0.5
#d <- d[(d$score >= threshold | is.na(d$score)),]
# Non-hierarchical metrics.
tp <- nrow(d[d$category %in% c("TP"),])
fp <- nrow(d[d$category %in% c("FP"),])
fn <- nrow(d[d$category %in% c("FN"),])
p <- tp/(tp+fp)
r <- tp/(tp+fn)
f1 <- (2*p*r)/(p+r)
# Graph-based metrics.
sum <- sum(d[d$category %in% c("TP","FN"),]$similarity)
avg <- sum/(tp+fn)
pr <- avg
sum <- sum(d[d$category %in% c("TP","FP"),]$similarity)
avg <- sum/(tp+fp)
pp <- avg
pf1 <- (2*pp*pr)/(pr+pp)
n <- tp+fn
# Adding one for partial precision of just the 'false' positives.
sum <- sum(d[d$category %in% c("FP"),]$similarity)
avg <- sum/(fp)
pp_fp <- avg
line = paste(n,pp,pr,pf1,sep=",")
return(line)
}
# Function for finding an appropriate threshold for this method.
find_threshold <- function(dir,filename,ratio){
# Read in the data.
d <- read.csv(file=paste(dir,filename,sep=""), header=T, sep=",")
# Numerical data that is missing was marked as na in the file.
d[d=="na"] <- NA
d[d$category%in%c("FN"),"score"] <- NA
d$score <- as.numeric(as.character(d$score))
# Total number of chunks in the testing set.
num_chunks <- length(unique(d$chunk))
# What's the expected number of terms in this ontology?
expected <- num_chunks*ratio
# What threshold on the predicted term's scores would leave that many?
d_pred <- d[d$category %in% c("FP","TP"),]
sorted_pred_scores <- sort(d_pred$score, decreasing=T, na.last=NA)
threshold <- sorted_pred_scores[ceiling(expected)]
return(threshold)
}
# Get a csv with metrics for a particular semantic annotation method.
sink("/Users/irbraun/Desktop/metrics.csv")
# testing the word embedding stuff for the nb model.
cat(paste("n,pp,pr,pf1\n"))
dir <- "/Users/irbraun/NetBeansProjects/term-mapping/path/annotators/naive/"
f1 <- "output_pato/test_eval.csv"
f2 <- "output_po/test_eval.csv"
cat(paste(table_row(dir,f1),"\n",sep=""))
cat(paste(table_row(dir,f2),"\n",sep=""))
# Normalization function for numerical scores.
range01 <- function(x, ...){(x - min(x, ...)) / (max(x, ...) - min(x, ...))}
# Function for creating a string for input into a table.
read <- function(dir,filename){
# Read in the data.
d <- read.csv(file=paste(dir,filename,sep=""), header=T, sep=",")
return(d)
}
# Function for creating a string for input into a table.
table_row <- function(dir,filename){
# Read in the data.
d <- read.csv(file=paste(dir,filename,sep=""), header=T, sep=",")
# Numerical data that is missing was marked as na in the file.
d[d=="na"] <- NA
# Check this stuff, only applies when a score was return from the concept mapping method.
#d$score <- as.numeric(as.character(d$score))
#d$score <- range01(d$score, na.rm=T)
#d[d$category%in%c("FN"),"score"] <- NA
#threshold = 0.5
#d <- d[(d$score >= threshold | is.na(d$score)),]
# Non-hierarchical metrics.
tp <- nrow(d[d$category %in% c("TP"),])
fp <- nrow(d[d$category %in% c("FP"),])
fn <- nrow(d[d$category %in% c("FN"),])
p <- tp/(tp+fp)
r <- tp/(tp+fn)
f1 <- (2*p*r)/(p+r)
# Graph-based metrics.
sum <- sum(d[d$category %in% c("TP","FN"),]$similarity)
avg <- sum/(tp+fn)
pr <- avg
sum <- sum(d[d$category %in% c("TP","FP"),]$similarity)
avg <- sum/(tp+fp)
pp <- avg
pf1 <- (2*pp*pr)/(pr+pp)
n <- tp+fn
# Adding one for partial precision of just the 'false' positives.
sum <- sum(d[d$category %in% c("FP"),]$similarity)
avg <- sum/(fp)
pp_fp <- avg
line = paste(n,pp,pr,pf1,sep=",")
return(line)
}
# Function for finding an appropriate threshold for this method.
find_threshold <- function(dir,filename,ratio){
# Read in the data.
d <- read.csv(file=paste(dir,filename,sep=""), header=T, sep=",")
# Numerical data that is missing was marked as na in the file.
d[d=="na"] <- NA
d[d$category%in%c("FN"),"score"] <- NA
d$score <- as.numeric(as.character(d$score))
# Total number of chunks in the testing set.
num_chunks <- length(unique(d$chunk))
# What's the expected number of terms in this ontology?
expected <- num_chunks*ratio
# What threshold on the predicted term's scores would leave that many?
d_pred <- d[d$category %in% c("FP","TP"),]
sorted_pred_scores <- sort(d_pred$score, decreasing=T, na.last=NA)
threshold <- sorted_pred_scores[ceiling(expected)]
return(threshold)
}
# Get a csv with metrics for a particular semantic annotation method.
sink("/Users/irbraun/Desktop/metrics.csv")
# testing the word embedding stuff for the nb model.
cat(paste("n,pp,pr,pf1\n"))
dir <- "/Users/irbraun/NetBeansProjects/term-mapping/path/annotators/naive/"
f1 <- "output_pato/test_eval.csv"
f2 <- "output_po/test_eval.csv"
cat(paste(table_row(dir,f1),"\n",sep=""))
cat(paste(table_row(dir,f2),"\n",sep=""))
# testing the word embedding stuff for the nb model.
cat(paste("n,pp,pr,pf1\n"))
dir <- "/Users/irbraun/NetBeansProjects/term-mapping/path/annotators/naive/"
f1 <- "output_pato/test_eval.csv"
f2 <- "output_po/test_eval.csv"
cat(paste(table_row(dir,f1),"\n",sep=""))
cat(paste(table_row(dir,f2),"\n",sep=""))
# testing the word embedding stuff for the nb model.
cat(paste("\nn,pp,pr,pf1\n"))
dir <- "/Users/irbraun/NetBeansProjects/term-mapping/path/annotators/naive/"
f1 <- "output_pato/test_eval.csv"
f2 <- "output_po/test_eval.csv"
cat(paste(table_row(dir,f1),"\n",sep=""))
cat(paste(table_row(dir,f2),"\n",sep=""))
# testing the word embedding stuff for the nb model.
cat(paste("\nn,pp,pr,pf1\n"))
dir <- "/Users/irbraun/NetBeansProjects/term-mapping/path/annotators/naive/"
f1 <- "output_pato/test_eval.csv"
f2 <- "output_po/test_eval.csv"
cat(paste(table_row(dir,f1),"\n",sep=""))
cat(paste(table_row(dir,f2),"\n",sep=""))
# testing the word embedding stuff for the nb model.
cat(paste("\nn,pp,pr,pf1\n"))
dir <- "/Users/irbraun/NetBeansProjects/term-mapping/path/annotators/naive/"
f1 <- "output_pato/test_eval.csv"
f2 <- "output_po/test_eval.csv"
cat(paste(table_row(dir,f1),"\n",sep=""))
cat(paste(table_row(dir,f2),"\n",sep=""))
# testing the word embedding stuff for the nb model.
cat(paste("\nn,pp,pr,pf1\n"))
dir <- "/Users/irbraun/NetBeansProjects/term-mapping/path/annotators/naive/"
f1 <- "output_pato/test_eval.csv"
f2 <- "output_po/test_eval.csv"
cat(paste(table_row(dir,f1),"\n",sep=""))
cat(paste(table_row(dir,f2),"\n",sep=""))
